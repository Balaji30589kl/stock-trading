# ğŸ“Š TECHNICAL SUMMARY - AI Stock Prediction Platform
### Complete Project Documentation for Viva Preparation

---

## ğŸ¯ PROJECT OVERVIEW

**Project Name:** AI-Powered Stock Price Prediction & Trading Platform  
**Type:** Full-Stack Web Application with Machine Learning Integration  
**Architecture:** Microservices (3-Tier Architecture)  
**Tech Stack:** React.js, Node.js/Express, FastAPI, PyTorch, MongoDB

**Purpose:** A comprehensive stock trading platform that integrates AI/ML capabilities to predict next-day closing prices for stocks, helping traders make informed decisions.

---

## ğŸ—ï¸ SYSTEM ARCHITECTURE

### **Three-Tier Microservices Architecture**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     FRONTEND (Port 3000)                     â”‚
â”‚                  React.js Dashboard                          â”‚
â”‚  Login/Register, Portfolio, Orders, AI Predictions          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ REST API (HTTP/JSON)
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     BACKEND (Port 3002)                      â”‚
â”‚             Node.js + Express.js API Server                  â”‚
â”‚   Authentication, Portfolio, Orders, Forecast Gateway        â”‚
â”‚              JWT Tokens + MongoDB Database                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ REST API
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   AI SERVICE (Port 8000)                     â”‚
â”‚           FastAPI + PyTorch LSTM Model                       â”‚
â”‚  Stock Data Fetching, Model Training, Price Prediction       â”‚
â”‚            Artifact Caching (model.pt, scaler.json)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Communication Flow:**
1. User interacts with React frontend
2. Frontend sends authenticated requests to backend
3. Backend validates JWT tokens and routes forecast requests to AI service
4. AI service fetches data from Yahoo Finance, trains/loads LSTM model, returns predictions
5. Response flows back through backend to frontend for visualization

---

## ğŸ¤– AI SERVICE - DETAILED TECHNICAL BREAKDOWN

### **Technology Stack**
- **Framework:** FastAPI (Python web framework)
- **ML Library:** PyTorch 2.4.0
- **Data Source:** yfinance (Yahoo Finance API)
- **Model Type:** LSTM (Long Short-Term Memory) Neural Network
- **Data Processing:** NumPy 2.0.1, Pandas 2.2.2

### **Model Architecture: LSTM Regressor**

**File:** `model.py` (20 lines, 559 bytes)

```python
class LSTMRegressor(nn.Module):
    def __init__(self, input_size=1, hidden_size=64, num_layers=2, dropout=0.2)
```

**Architecture Details:**
- **Model Type:** Recurrent Neural Network (LSTM)
- **Input Size:** 1 (single feature: closing price)
- **Hidden Layer Size:** 64 neurons
- **Number of LSTM Layers:** 2 (stacked LSTM)
- **Dropout Rate:** 0.2 (20% - prevents overfitting)
- **Output Layer:** Fully connected (Linear) layer â†’ 1 output (next day price)

**Total Parameters Calculation:**
- LSTM Layer 1: 4 Ã— (1 Ã— 64 + 64 Ã— 64 + 64) = 16,896 parameters
- LSTM Layer 2: 4 Ã— (64 Ã— 64 + 64 Ã— 64 + 64) = 33,024 parameters
- Dropout: 0 parameters (regularization only)
- FC Layer: 64 Ã— 1 + 1 = 65 parameters
**Total: ~50,000 trainable parameters**

**Why LSTM?**
1. **Sequential Data:** Stock prices are time-series data with temporal dependencies
2. **Memory Capability:** LSTM remembers long-term patterns (unlike simple RNNs)
3. **Gradient Stability:** Solves vanishing gradient problem through cell state
4. **Context Awareness:** Uses gates (forget, input, output) to learn what to remember

---

### **Dataset Specifications**

**Data Source:** Yahoo Finance via `yfinance` library

**Dataset Details:**
- **Historical Period:** 2 years (730 days default)
- **Interval:** Daily (1d)
- **Feature:** Close price only (univariate time series)
- **Volume:** Typically 500-730 data points per stock
- **Train/Test Split:** 80/20 ratio
- **Lookback Window:** 60 days (model sees 60 previous days to predict day 61)

**Example Dataset Size:**
- Total data points: 500 days
- Lookback: 60 days
- Sequences created: 500 - 60 = 440 sequences
- Training sequences: 440 Ã— 0.8 = 352
- Testing sequences: 440 Ã— 0.2 = 88

**Data Preprocessing:**
1. **MinMax Scaling:** Normalize to [0, 1] range
2. **Sequence Creation:** Sliding window of 60 days
3. **Train/Test Split:** Chronological (first 80% train, last 20% test)

---

### **AI Service File-by-File Breakdown**

#### **1. main.py** (55 lines, 1.3 KB)
**Purpose:** FastAPI application entry point and REST API endpoints

**What it does:**
- Defines FastAPI app with title "AI Stock Prediction Service"
- Creates Pydantic models for request/response validation
- Exposes `/predict` endpoint (POST) and `/health` endpoint (GET)
- Handles HTTP requests and error responses (400, 500 status codes)

**Why we need it:**
- Serves as the web server interface for the AI model
- Validates incoming requests (symbol length 1-15 chars)
- Converts HTTP requests to Python function calls
- Returns structured JSON responses with predictions

**Key Components:**
- `PredictRequest`: Input validation (symbol field)
- `PredictResponse`: Output schema (symbol, predicted_close, history, etc.)
- Error handling: ValueError â†’ 400, Generic Exception â†’ 500

#### **2. inference.py** (104 lines, 3.5 KB)
**Purpose:** Core prediction logic and model orchestration

**What it does:**
- `_load_or_train()`: Checks cache â†’ loads artifacts â†’ trains new model if needed
- `predict_next_close()`: Main prediction function with full workflow
- Manages in-memory `MODEL_CACHE` dictionary for performance
- Fetches stock metadata (name, history) and combines with prediction

**Why we need it:**
- Implements the prediction pipeline end-to-end
- Optimizes performance through caching (avoids retraining)
- Handles model lifecycle (load existing or train new)
- Prepares comprehensive response with all required data

**Workflow:**
1. Check if model exists in memory cache
2. If not, try loading from disk artifacts
3. If not found, fetch data and train new model
4. Make prediction using loaded/trained model
5. Fetch additional data (stock name, 6-month history)
6. Return complete response

#### **3. trainer.py** (167 lines, 5.0 KB)
**Purpose:** Model training, evaluation, and artifact management

**What it does:**
- `train_model()`: Trains LSTM with Adam optimizer (15 epochs, batch size 64)
- `evaluate_rmse()`: Calculates Root Mean Square Error on test set
- `save_artifacts()`: Persists model.pt, scaler.json, metadata.json to disk
- `load_artifacts()`: Loads saved model and parameters from disk
- `set_deterministic_seed()`: Ensures reproducible results (seed=42)

**Why we need it:**
- Separates training logic from inference logic (modularity)
- Provides artifact persistence for production deployment
- Enables model evaluation with RMSE metric
- Ensures reproducibility with seed setting

**Training Hyperparameters:**
- **Epochs:** 15 iterations over training data
- **Batch Size:** 64 sequences per update
- **Learning Rate:** 0.001 (Adam optimizer)
- **Loss Function:** MSE (Mean Squared Error)
- **Optimizer:** Adam (adaptive learning rate)

**Artifact Structure:**
```
artifacts/
  â””â”€â”€ AAPL/
      â”œâ”€â”€ model.pt         # PyTorch model weights
      â”œâ”€â”€ scaler.json      # MinMax scaler params (vmin, vmax)
      â””â”€â”€ metadata.json    # Training metadata (RMSE, dates, sizes)
```

#### **4. data.py** (116 lines, 3.6 KB)
**Purpose:** Data fetching, preprocessing, and transformation

**What it does:**
- `fetch_close_series()`: Downloads historical close prices from Yahoo Finance
- `fetch_stock_name()`: Retrieves company full name (e.g., "Apple Inc.")
- `fetch_history()`: Gets 6-month price history for charting
- `create_sequences()`: Converts time series to supervised learning format
- `minmax_scale()`: Normalizes data to [0, 1] range
- `prepare_data()`: Complete preprocessing pipeline

**Why we need it:**
- Interfaces with external data source (Yahoo Finance)
- Transforms raw data into ML-ready format
- Handles edge cases (empty data, MultiIndex columns)
- Provides reusable data processing functions

**Key Functions:**
- **Normalization Formula:** `(value - min) / (max - min)`
- **Sequence Creation:** Rolling window of size 60
- **MultiIndex Handling:** Flattens yfinance column structure

#### **5. model.py** (20 lines, 559 bytes)
**Purpose:** Neural network architecture definition

**What it does:**
- Defines `LSTMRegressor` class inheriting from `nn.Module`
- Initializes LSTM layers and fully connected output layer
- Implements forward pass logic

**Why we need it:**
- Encapsulates model architecture in reusable class
- Follows PyTorch conventions for model definition
- Allows easy modification of architecture parameters

**Architecture:**
```
Input (batch, 60, 1) 
    â†’ LSTM Layer 1 (64 hidden units)
    â†’ Dropout (0.2)
    â†’ LSTM Layer 2 (64 hidden units)
    â†’ Dropout (0.2)
    â†’ Take last timestep output
    â†’ Fully Connected Layer (64 â†’ 1)
    â†’ Output (batch, 1)
```

#### **6. utils.py** (3 lines, 78 bytes)
**Purpose:** Package initialization and export management

**What it does:**
- Exports `predict_next_close` as the main public API
- Makes inference function importable as `from ai_service import predict_next_close`

**Why we need it:**
- Defines clean public API for the package
- Allows backend to import without knowing internal structure

#### **7. requirements.txt** (7 lines, 113 bytes)
**Purpose:** Python dependency specification

**What it does:**
- Lists all required packages with exact versions
- Enables reproducible environment setup

**Dependencies:**
- **fastapi 0.115.0:** Modern web framework
- **uvicorn 0.30.6:** ASGI server to run FastAPI
- **yfinance 0.2.43:** Yahoo Finance data API
- **pandas 2.2.2:** Data manipulation
- **numpy 2.0.1:** Numerical operations
- **torch 2.4.0:** Deep learning framework
- **pydantic 2.8.2:** Data validation

---

## ğŸ”§ BACKEND SERVICE - TECHNICAL DETAILS

### **Technology Stack**
- **Runtime:** Node.js (JavaScript runtime)
- **Framework:** Express.js 4.18.3
- **Database:** MongoDB with Mongoose 8.2.1 ODM
- **Authentication:** JWT (jsonwebtoken 9.0.3) + bcryptjs 3.0.3
- **HTTP Client:** Axios 1.6.7

### **Architecture Pattern:** MVC (Model-View-Controller)

**Directory Structure:**
```
backend/src/
â”œâ”€â”€ index.js                    # Entry point, server initialization
â”œâ”€â”€ config/
â”‚   â””â”€â”€ database.js            # MongoDB connection
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ User.js                # User schema (name, email, password)
â”‚   â”œâ”€â”€ HoldingsModel.js       # Stock holdings
â”‚   â”œâ”€â”€ OrdersModel.js         # Order history
â”‚   â””â”€â”€ PositionsModel.js      # Open positions
â”œâ”€â”€ controllers/
â”‚   â”œâ”€â”€ auth.controller.js     # Login/register logic
â”‚   â”œâ”€â”€ forecastController.js  # AI prediction proxy
â”‚   â”œâ”€â”€ portfolio.controller.js
â”‚   â””â”€â”€ orders.controller.js
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ auth.service.js        # JWT generation/validation
â”‚   â””â”€â”€ aiService.js           # HTTP client for AI service
â”œâ”€â”€ middleware/
â”‚   â””â”€â”€ auth.middleware.js     # JWT token verification
â””â”€â”€ routes/
    â”œâ”€â”€ auth.routes.js         # /register, /login
    â”œâ”€â”€ forecastRoutes.js      # /forecast (protected)
    â”œâ”€â”€ portfolio.routes.js
    â””â”€â”€ orders.routes.js
```

### **Key Responsibilities**

1. **Authentication:**
   - User registration with password hashing (bcrypt rounds=10)
   - Login with JWT token generation (expires 7 days)
   - Token verification middleware for protected routes

2. **Database Management:**
   - MongoDB connection with Mongoose ODM
   - User schema: name, email, password, createdAt
   - Portfolio schemas: holdings, positions, orders

3. **AI Service Integration:**
   - HTTP proxy to AI service (axios)
   - Request validation (symbol length â‰¤ 15 chars)
   - Timeout handling (30 seconds default)
   - Error translation (AI 500 â†’ Backend 502)

4. **API Gateway:**
   - Routes all frontend requests
   - CORS enabled for cross-origin requests
   - Body parsing (JSON)

### **Environment Variables (.env)**
```
PORT=3002
MONGO_URL=mongodb://127.0.0.1/zerodha
JWT_SECRET=your_secret_key
AI_SERVICE_URL=http://localhost:8000
AI_SERVICE_TIMEOUT=30000
```

---

## ğŸ¨ FRONTEND - TECHNICAL DETAILS

### **Technology Stack**
- **Library:** React 18.2.0 (functional components + hooks)
- **Routing:** React Router DOM 6.22.2
- **HTTP Client:** Axios 1.6.7
- **UI Framework:** Material-UI (@mui/material 5.15.11)
- **Charting:** Chart.js 4.4.2, react-chartjs-2 5.2.0
- **Build Tool:** Create React App (react-scripts 5.0.1)

### **Component Architecture**

**Main Components:**
```
dashboard/src/components/
â”œâ”€â”€ Apps.js                  # Main app router
â”œâ”€â”€ Login.js                 # Login form
â”œâ”€â”€ Register.js              # Registration form
â”œâ”€â”€ Dashboard.js             # Main dashboard layout
â”œâ”€â”€ Home.js                  # Dashboard home page
â”œâ”€â”€ TopBar.js                # Navigation bar
â”œâ”€â”€ Menu.js                  # Sidebar menu
â”œâ”€â”€ Holdings.js              # Portfolio holdings
â”œâ”€â”€ Positions.js             # Open positions
â”œâ”€â”€ Orders.js                # Order history
â”œâ”€â”€ Funds.js                 # Funds management
â”œâ”€â”€ WatchList.js             # Stock watchlist
â”œâ”€â”€ AiPage.js                # AI Predictions page
â”œâ”€â”€ PredictionForm.js        # Stock symbol input form
â”œâ”€â”€ PredictionResult.js      # Prediction display with chart
â”œâ”€â”€ ProtectedRoute.js        # Route authentication wrapper
â”œâ”€â”€ BuyActionWindow.js       # Buy order modal
â”œâ”€â”€ SellActionWindow.js      # Sell order modal
â”œâ”€â”€ Summary.js               # Portfolio summary
â”œâ”€â”€ VerticalGraph.js         # Vertical chart component
â””â”€â”€ DoughnoutChart.js        # Doughnut chart component
```

### **State Management:** React Hooks
- `useState`: Local component state
- `useEffect`: Side effects (API calls, data fetching)
- `useMemo`: Memoized chart computations
- `useContext`: Global state (GeneralContext for user data)

### **AI Prediction Features (v1.1)**

**PredictionResult.js - Professional Chart:**
- **SVG-based chart** (600Ã—300px, responsive)
- **Axes:** Y-axis (price labels), X-axis (date labels)
- **Grid lines:** Horizontal dashed lines for readability
- **Gradient fill:** Area under the line chart
- **Trend indicator:** Upward ğŸ“ˆ, Downward ğŸ“‰, or Stable â¡ï¸
- **Data table:** Last 15 days of price history

**Chart Specifications:**
- ViewBox: 600Ã—300 (16:9 aspect ratio)
- Padding: 60px left, 40px bottom (for labels)
- 5 Y-axis labels (min to max price)
- 6 X-axis labels (evenly distributed dates)
- Line: 2.5px stroke, blue (#3b82f6)
- Background gradient: Blue with 20% â†’ 2% opacity fade

---

## ğŸ“Š COMPLETE DATA FLOW EXAMPLE

**User Action:** User enters "AAPL" in AI Prediction page

### **Step-by-Step Flow:**

1. **Frontend (React):**
   ```javascript
   // User clicks search
   axios.post('/forecast', { symbol: 'AAPL' })
   // Adds JWT token from localStorage
   headers: { Authorization: 'Bearer <token>' }
   ```

2. **Backend (Express):**
   ```javascript
   // Middleware validates JWT token
   verifyToken() â†’ next()
   // Validates symbol length
   validateForecastRequest() â†’ next()
   // Forwards to AI service
   aiService.getForecast('AAPL')
   ```

3. **AI Service (FastAPI):**
   ```python
   # Receives POST /predict
   predict_next_close('AAPL')
   
   # Check cache/artifacts
   if not in cache:
       # Fetch data from Yahoo Finance (2 years)
       df = yf.download('AAPL', period='2y')
       # 500 data points â†’ 440 sequences (60 lookback)
       
       # Train model
       train_model(x_train=352, y_train=352)
       # 15 epochs, batch_size=64, lr=0.001
       
       # Evaluate on test set
       rmse = evaluate_rmse(x_test=88, y_test=88)
       # Example RMSE: 2.45
       
       # Save artifacts
       save_artifacts('AAPL/', model.pt, scaler.json, metadata.json)
   
   # Make prediction
   last_60_days = [230.1, 231.5, ..., 264.58]
   scaled = normalize(last_60_days, vmin=200.3, vmax=265.1)
   prediction = model.forward(scaled)
   predicted = denormalize(prediction) # 260.96
   
   # Fetch additional data
   stock_name = "Apple Inc."
   history = fetch_6_month_history() # 126 data points
   
   # Return response
   return {
     symbol: "AAPL",
     predicted_close: 260.96,
     last_close: 264.58,
     stock_name: "Apple Inc.",
     history: [...126 data points...],
     metrics: { rmse: 2.45 }
   }
   ```

4. **Backend (Express):**
   ```javascript
   // Receives AI response
   // Wraps in success envelope
   return {
     success: true,
     data: { ...AI response... }
   }
   ```

5. **Frontend (React):**
   ```javascript
   // Receives response
   setPrediction(response.data.data)
   
   // Computes signal
   change% = (260.96 - 264.58) / 264.58 = -1.37%
   signal = "SELL" (< 1%)
   
   // Generates chart
   // 126 history points â†’ SVG path
   // Y-axis: 5 labels from min to max
   // X-axis: 6 date labels
   
   // Renders UI
   <PredictionResult prediction={...} />
   ```

**Total Time:** ~20-30 seconds (first request, includes training)  
**Cached Request:** ~2-3 seconds (model already trained)

---

## ğŸ” SECURITY FEATURES

1. **Authentication:**
   - Password hashing with bcrypt (10 salt rounds)
   - JWT tokens with 7-day expiration
   - Protected routes require valid token

2. **Input Validation:**
   - Symbol length: 1-15 characters
   - Email format validation
   - Password strength requirements

3. **API Security:**
   - CORS configuration
   - Request timeouts (30s AI service)
   - Error message sanitization

---

## âš¡ PERFORMANCE OPTIMIZATIONS

1. **Model Caching:**
   - In-memory cache (MODEL_CACHE dictionary)
   - Disk persistence (artifacts folder)
   - Avoids retraining for same stock

2. **Frontend:**
   - useMemo for chart computations
   - Conditional rendering (loading states)
   - Responsive SVG charts (scale to container)

3. **Backend:**
   - Connection pooling (MongoDB)
   - Request timeout handling
   - Async/await patterns

---

## ğŸ“ˆ MODEL EVALUATION METRICS

**Metric:** RMSE (Root Mean Square Error)

**Formula:** 
```
RMSE = âˆš(Î£(predicted - actual)Â² / n)
```

**Interpretation:**
- Lower RMSE = Better prediction accuracy
- Measured in same units as price (â‚¹ or $)
- Example: RMSE = 2.45 means predictions off by Â±â‚¹2.45 on average

**Why RMSE?**
1. Penalizes large errors more than MAE
2. Easy to interpret (same unit as target)
3. Standard metric for regression problems

---

## ğŸš€ DEPLOYMENT CONSIDERATIONS

**Development Setup:**
1. MongoDB running on localhost:27017
2. AI service on port 8000 (uvicorn)
3. Backend on port 3002 (nodemon)
4. Frontend on port 3000 (react-scripts)

**Production Enhancements Needed:**
- Docker containerization
- Environment-specific configs
- Database migrations
- HTTPS/SSL certificates
- Rate limiting
- Logging/monitoring
- CI/CD pipeline

---

## ğŸ“ VIVA PREPARATION - KEY TALKING POINTS

### **Why This Tech Stack?**

1. **React:** Component-based, virtual DOM, rich ecosystem
2. **Node.js:** JavaScript everywhere, async I/O, npm packages
3. **FastAPI:** Fast (ASGI), auto-documentation, Pydantic validation
4. **PyTorch:** Flexible, Pythonic, research-grade ML framework
5. **MongoDB:** Document-based, flexible schema, JSON-like storage
6. **LSTM:** Sequential data, long-term memory, handles vanishing gradients

### **Design Decisions:**

1. **Microservices:** Separation of concerns, independent scaling, language flexibility
2. **60-day lookback:** Balance between context and overfitting (too long â†’ overfitting)
3. **80/20 split:** Standard ML practice, enough test data for validation
4. **MinMax scaling:** Preserves relationships, suits neural networks better than standardization
5. **Artifact persistence:** Avoids retraining, faster predictions, production-ready

### **Limitations & Future Work:**

1. **Single feature:** Only uses close price (could add volume, indicators)
2. **No sentiment analysis:** Could integrate news/social media
3. **Limited stocks:** Works best with liquid, high-volume stocks
4. **Day-ahead only:** Could extend to multi-day predictions
5. **No backtesting:** Should validate strategy profitability

---

## ğŸ“š TECHNICAL GLOSSARY

- **LSTM:** Long Short-Term Memory (type of RNN)
- **Sequence:** Array of consecutive time steps (e.g., 60 days)
- **Epoch:** One complete pass through training data
- **Batch:** Subset of training data processed together
- **Dropout:** Randomly ignore neurons during training (prevents overfitting)
- **RMSE:** Root Mean Square Error (prediction accuracy metric)
- **JWT:** JSON Web Token (authentication standard)
- **CORS:** Cross-Origin Resource Sharing (browser security)
- **ODM:** Object Document Mapper (MongoDB abstraction)
- **REST:** Representational State Transfer (API architecture)

---

## ğŸ“Š PROJECT STATISTICS

**Lines of Code:**
- AI Service: ~472 lines (Python)
- Backend: ~800 lines (JavaScript)
- Frontend: ~2000 lines (JavaScript/JSX)
- **Total: ~3300 lines**

**Files:**
- AI Service: 7 files
- Backend: ~25 files
- Frontend: ~30 files
- **Total: ~62 files**

**Dependencies:**
- Python: 7 packages
- Node.js Backend: 11 packages
- React Frontend: 14 packages

**Model Training Time:**
- First prediction: 20-30 seconds (download data + train)
- Subsequent requests: 2-3 seconds (cached model)
- Training epochs: 15 iterations
- Parameters trained: ~50,000

---

**Document Created:** For VIVA Preparation  
**Version:** 1.1 (Enhanced with professional charts)  
**Last Updated:** February 21, 2026

---

## ğŸ¯ SUGGESTED VIVA QUESTIONS & ANSWERS

### Q1: Why did you choose LSTM over other models like ARIMA or simple RNNs?

**Answer:** We chose LSTM over ARIMA because:
1. **Non-linearity:** Stock prices have complex non-linear patterns that ARIMA (linear model) cannot capture
2. **Long-term dependencies:** LSTM remembers information for longer periods through cell state, whereas simple RNNs suffer from vanishing gradients
3. **Feature flexibility:** Neural networks can easily incorporate multiple features in future (volume, indicators), while ARIMA is primarily univariate
4. **Modern approach:** Deep learning is state-of-the-art for time series, and LSTM is proven effective for sequential data

### Q2: How does the 60-day lookback window affect model performance?

**Answer:** The 60-day (approximately 3-month) lookback is chosen because:
1. **Sufficient context:** Captures quarterly patterns and short-term trends
2. **Prevents overfitting:** Too long (e.g., 200 days) makes model memorize noise
3. **Market relevance:** Recent 3 months are most relevant for next-day prediction
4. **Data efficiency:** With 500 total points, 60 lookback creates 440 sequencesâ€”enough for training

If we increase to 120 days, we get fewer sequences (380), reducing training data. If we decrease to 30 days, the model lacks sufficient context.

### Q3: Explain the MinMax scaling formula and why use it instead of standardization?

**Answer:** 
**Formula:** `scaled = (value - min) / (max - min)`  
This maps values to [0, 1] range.

**Why MinMax over Standardization (z-score)?**
1. **Bounded range:** Neural networks with sigmoid/tanh activations work better with [0,1] inputs
2. **Preserves relationships:** Maintains the relative distances between values
3. **No assumptions:** Doesn't assume normal distribution (stock prices aren't normally distributed)
4. **Interpretation:** Easy to reverse (denormalization) for final prediction

**Disadvantage:** Sensitive to outliers (extreme values stretch the scale), but we address this by using 2-year historical data which smooths extremes.

### Q4: What is the purpose of dropout and how does it prevent overfitting?

**Answer:** Dropout (0.2 = 20%) randomly ignores 20% of neurons during each training iteration.

**How it prevents overfitting:**
1. **Forces redundancy:** Network can't rely on specific neurons, learns robust features
2. **Ensemble effect:** Each training step uses a different sub-network, like training multiple models
3. **Reduces co-adaptation:** Neurons don't develop complex dependencies on each other

**During inference:** Dropout is turned off (model.eval()), all neurons are used with weighted outputs.

**Why 0.2?** Common practice for small networks. Too high (>0.5) causes underfitting, too low (<0.1) doesn't help.

### Q5: How does your caching mechanism improve performance?

**Answer:** We implement two-level caching:

**1. Memory Cache (Python dictionary):**
```python
MODEL_CACHE = {'AAPL': {model, vmin, vmax, metadata}}
```
- First request: 25s (download + train)
- Subsequent requests: 2s (direct memory access)
- Persists while server runs

**2. Disk Cache (artifacts/):**
```
artifacts/AAPL/
  â”œâ”€â”€ model.pt (50K parameters)
  â”œâ”€â”€ scaler.json (vmin, vmax)
  â””â”€â”€ metadata.json (RMSE, trained_at)
```
- Survives server restarts
- Avoids retraining after deployment
- Load time: 1-2s (faster than training)

**Benefit:** 90% reduction in response time for popular stocks.

### Q6: Explain your train-test split strategy.

**Answer:** We use **chronological 80/20 split**:

**Example:**
- Total data points: 500 days (2023-01-01 to 2024-11-15)
- Lookback: 60 days
- Sequences created: 440
- Train: First 352 sequences (days 1-412)
- Test: Last 88 sequences (days 413-500)

**Why chronological (not random)?**
1. **Prevents data leakage:** Can't use future to predict past
2. **Realistic evaluation:** Tests on most recent data (what we'll actually predict)
3. **Respects time dependency:** Maintains temporal order

**Random split would cheat:** Model sees 2024 data during training, then tested on 2023 â†’ unrealistic.

### Q7: What does RMSE of 2.45 mean for a â‚¹200 stock?

**Answer:** RMSE = 2.45 means:
- Average prediction error: Â±â‚¹2.45
- Percentage error: 2.45/200 = 1.2%
- For a â‚¹200 stock predicting â‚¹202, actual price likely â‚¹199-â‚¹205

**Is this good?**
- Stock volatility: Typical daily change is 1-3%
- Our model's 1.2% error is competitive
- For day trading (small margins), need to combine with other signals
- For swing trading (multi-day), this is acceptable

**Comparison:** Random walk model (predict today's price = yesterday's price) often has RMSE of 3-5% for volatile stocks.

### Q8: How does JWT authentication work in your system?

**Answer:** 
**Registration/Login Flow:**
1. User submits email + password
2. Backend hashes password with bcrypt (10 salt rounds)
3. Stores hash in MongoDB (NOT plain password)
4. Generates JWT token:
```javascript
jwt.sign(
  { userId: user._id, email: user.email },
  secret_key,
  { expiresIn: '7d' }
)
```
5. Frontend stores token in localStorage
6. Every API request includes: `Authorization: Bearer <token>`

**Token Verification:**
1. Middleware extracts token from header
2. Verifies signature using same secret_key
3. Checks expiration (7 days)
4. If valid, continues to route; else returns 401 Unauthorized

**Security:** Token is signed (tamper-proof), not encrypted. Never store sensitive data in token.

### Q9: Why separate AI service from backend instead of one monolith?

**Answer:** Microservices architecture provides:

**1. Language Flexibility:**
- AI: Python (PyTorch, NumPy, pandas = ML ecosystem)
- Backend: JavaScript (Node.js = async, npm packages)
- Each uses best language for its purpose

**2. Independent Scaling:**
- AI service is CPU-intensive (model training)
- Backend is I/O-intensive (database, API calls)
- Can scale AI service on CPU-heavy servers, backend on lightweight instances

**3. Development Velocity:**
- AI team works independently on model improvements
- Backend team adds features without touching ML code
- Clear API contract (`/predict` endpoint)

**4. Fault Isolation:**
- If AI service crashes (OOM during training), backend stays alive
- Users can still access portfolio, orders

**5. Technology Upgrades:**
- Upgrade PyTorch 2.4 â†’ 3.0 without touching Node.js
- Migrate backend to TypeScript without touching Python

### Q10: Explain the forward pass of your LSTM model step-by-step.

**Answer:**

**Input Shape:** (1, 60, 1) = [batch=1, sequence=60 days, features=1 price]

**Step-by-step:**
```
1. Input Layer: [1, 60, 1]
   â†“
2. LSTM Layer 1 (64 hidden units):
   - For each of 60 timesteps:
     * Forget gate: Decide what to forget from cell state
     * Input gate: Decide what new info to add
     * Cell state: Update long-term memory
     * Output gate: Decide what to output
   - Output shape: [1, 60, 64]
   â†“
3. Dropout (0.2): Randomly zero 20% of neurons
   - Shape: [1, 60, 64]
   â†“
4. LSTM Layer 2 (64 hidden units):
   - Same gate operations
   - Output shape: [1, 60, 64]
   â†“
5. Dropout (0.2)
   - Shape: [1, 60, 64]
   â†“
6. Take Last Timestep: [:, -1, :]
   - Only use output from day 60 (most recent)
   - Shape: [1, 64]
   â†“
7. Fully Connected Layer: 64 â†’ 1
   - Matrix multiply: [1, 64] Ã— [64, 1]
   - Output shape: [1, 1]
   â†“
8. Output: Single predicted price (scaled 0-1)
   - Denormalize: prediction Ã— (max-min) + min
   - Final: â‚¹260.96
```

**Key insight:** LSTM processes sequence sequentially, but we only use the final hidden state (day 60) for prediction, as it encodes all 60 days of context.

---

### ğŸ† BEST PRACTICES DEMONSTRATED

1. âœ… **Separation of Concerns:** Model, data, inference, API in separate files
2. âœ… **Error Handling:** Try-catch blocks, HTTP status codes, fallbacks
3. âœ… **Reproducibility:** Deterministic seed (42) for consistent results
4. âœ… **Caching:** Memory + disk caching for performance
5. âœ… **Validation:** Pydantic models, middleware validation
6. âœ… **Security:** JWT tokens, password hashing, CORS
7. âœ… **Documentation:** README, comments, metadata
8. âœ… **Version Control:** .gitignore, clean commits
9. âœ… **User Experience:** Loading states, error messages, professional charts
10. âœ… **Scalability:** Microservices, artifact persistence, async operations

---

**Good luck with your viva! ğŸ“**
